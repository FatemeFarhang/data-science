You have to choose *at least one* of the following tasks. Each task is based on a research paper that has been linked in the title.

# Task No. 1: Data Crawling ([Paper](https://www.mdpi.com/2076-3417/11/14/6497))
Implemenet a crawling system that can collect the information listed in 4.1. You do not need to follow the architecture the paper has suggested in 3.2.1. You will only be evaluated on your successful collection of the target data. You are free to choose the language and the framework. How you want to store this data, e.g. in a SQL database or in CSV files, is up to you. You can also choose any social network that you feel more comfortable with.

# Task No. 2: Data Analysis ([Paper](https://www.mdpi.com/2076-3417/11/14/6497))
Using the custom metrics introduced in 4.2, measure the impact of users on a social network using the 3 introduced metrics. If you didn't choose to perform Task No. 1, generate a toy dataset and analyze that. You are free to choose the language and the framework. However, if you choose to go with Python, handing in the code and the results in a single Jupyter Notebook is a plus.

# Task No. 3: Data Analysis ([Paper](https://hal.archives-ouvertes.fr/hal-02120859))
Replicate the experiments on one (or more, if needed) freely available dataset. This one is pretty straightforward! You are free to choose the language and the framework. However, if you choose to go with Python, handing in the code and the results in a single Jupyter Notebook is a plus.
